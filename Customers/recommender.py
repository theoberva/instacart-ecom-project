# -*- coding: utf-8 -*-
"""bpr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13LZ7aEahprcdvetE6N8D-kUIr5hPbuub
"""

from google.colab import drive
drive.mount('/content/drive')

!mkdir ~/.kaggle
!touch ~/.kaggle/kaggle.json

api_token = {"username":"theobervanakis","key":"74de48417bd53eae60e2b46c03500945"}

import json

with open('/root/.kaggle/kaggle.json', 'w') as file:
    json.dump(api_token, file)

!chmod 600 ~/.kaggle/kaggle.json

import os
import zipfile
import kaggle

# Download dataset from Kaggle
if not os.path.exists('dataset/instacart-market-basket-analysis.zip'):
    os.system('kaggle competitions download -c instacart-market-basket-analysis -p dataset/')

# Unzip download
ZIPFILE_PATH = os.path.join('dataset', 'instacart-market-basket-analysis.zip')

with zipfile.ZipFile(ZIPFILE_PATH, 'r') as main_zipfile:
    main_zipfile.extractall(path=os.path.join('dataset', 'zip'))

# Unzip all wanted csv files
EXTRACTED_ZIPFILES_DIRECTORY = os.path.join('dataset', 'zip')

for filename in os.listdir(EXTRACTED_ZIPFILES_DIRECTORY):
    individual_zipfile_path = os.path.join(EXTRACTED_ZIPFILES_DIRECTORY, filename)
    if filename.endswith('.zip') and filename != 'sample_submission.csv.zip':
        with zipfile.ZipFile(individual_zipfile_path, 'r') as individual_zipfile:
            individual_zipfile.extractall(path=os.path.join('dataset', 'csv'))

os.remove(ZIPFILE_PATH)

# Spark is written in Scala. Soo we will require a Java Virtual Machine (JVM) to run it.
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
#  install Apache Spark 3.0.1 with Hadoop 2.7
!wget -P /content/drive/MyDrive/PySpark https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz
# unzip
!tar xf /content/drive/MyDrive/PySpark/spark-3.4.0-bin-hadoop3.tgz

# Set the environment path
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.0-bin-hadoop3"

!pip install -q findspark
!pip install pyspark
import findspark
findspark.init()

# Import SparkSession from pyspark.sql and create a SparkSession
from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local")\
        .appName("instacart_bpr_recommendations")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

spark

!pip install pyngrok

from pyngrok import ngrok

# Setup Spark UI
# Setting the authtoken
ngrok.kill()
NGROK_AUTH_TOKEN = "2QYzvFUSlUfljVMKegBFfjOGhfg_r1D7SJ9mUCcjMt31opJg"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Open an HTTP tunnel on port 4050
public_url = ngrok.connect(4050)
print("Your Spark UI is exposed at:", public_url)

# Read the CSV data
orders = spark.read.csv('dataset/csv/orders.csv', header=True, inferSchema=True)
order_products_prior = spark.read.csv('dataset/csv/order_products__prior.csv', header=True, inferSchema=True)
order_products_train = spark.read.csv('dataset/csv/order_products__train.csv', header=True, inferSchema=True)

order_products_prior.count()

!pip install lightfm
import numpy as np
from scipy.sparse import coo_matrix
from sklearn.preprocessing import LabelEncoder
from lightfm import LightFM
import pandas as pd

from pyspark.sql.functions import col, count
from pyspark.ml.recommendation import ALS
from pyspark.ml.feature import StringIndexer


# Join the orders with the order_products_total
merged_df = order_products_prior.join(orders, "order_id")

# Create a user-item interaction DataFrame
user_item_interaction = merged_df.groupby(['user_id', 'product_id']).count()

# StringIndexer to convert user_id and product_id to integers,
# it automatically assigns unique integer-based index values to each level of the categorical variables.
user_indexer = StringIndexer(inputCol="user_id", outputCol="user")
item_indexer = StringIndexer(inputCol="product_id", outputCol="item")

user_item_interaction = user_indexer.fit(user_item_interaction).transform(user_item_interaction)
user_item_interaction = item_indexer.fit(user_item_interaction).transform(user_item_interaction)

# Train the ALS model
als = ALS(maxIter=10, regParam=0.1, userCol="user", itemCol="item", ratingCol="count", coldStartStrategy="drop")
model = als.fit(user_item_interaction)

# If you want to generate recommendations
user_recs = model.recommendForAllUsers(10)

# Assume you have a DataFrame `users` with a single column "user" containing user IDs
# for which you want to generate recommendations.
users = spark.createDataFrame([(1,), (2,)], ["user"])

# Generate top 10 movie recommendations for a specified set of users
userSubsetRecs = model.recommendForUserSubset(users, 10)
userSubsetRecs.printSchema()

from pyspark.sql.functions import explode

# "explode" the recommendations array into multiple rows
userSubsetRecs = userSubsetRecs.select("user", explode("recommendations").alias("recommendation"))

# Split the recommendation structure into two separate columns: item and rating
userSubsetRecs = userSubsetRecs.select("user", "recommendation.*")
userSubsetRecs.show()

user_recs.show()

from pyspark.sql import functions as F

# Convert the 'recommendations' column to a JSON string representation
user_recs = user_recs.withColumn("recommendations_str", F.expr("to_json(recommendations)"))

# Reduce the number of partitions to 1 to save as a single CSV file
user_recs_single_partition = user_recs.coalesce(1)

# Select the 'user' and 'recommendations_str' columns and save as CSV with header
user_recs_single_partition.select("user", "recommendations_str").write.csv("dataset/agg/user_seg", header=True)

